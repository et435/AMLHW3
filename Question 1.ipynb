{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import io \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from numpy import linalg\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonRawText = open(\"amazon_cells_labelled.txt\").read()\n",
    "imdbRawText = open(\"imdb_labelled.txt\").read()\n",
    "yelpRawText = open(\"yelp_labelled.txt\").read()\n",
    "wnl = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are even, 500 positive reviews and 500 negative reviews per source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(inputLine,stopWords):\n",
    "    \n",
    "    noStops = \"\"\n",
    "    finalLine = inputLine.lower()\n",
    "    finalLine = finalLine.rstrip()\n",
    "    finalLine = finalLine.translate(None, string.punctuation)\n",
    "    for word in finalLine.split(\" \"):\n",
    "        if word != \"\" and all(ord(s) < 128 for s in word):\n",
    "            try:\n",
    "                part_of_speech = wn.synsets(word)[0].pos()\n",
    "            except:\n",
    "                part_of_speech = 'n'\n",
    "            lemmedWord = wnl.lemmatize(word,pos=part_of_speech)\n",
    "            lemmedWord = stemmer.stem(lemmedWord)\n",
    "            if lemmedWord not in stopWords:\n",
    "                noStops = noStops + word + \" \"\n",
    "    finalLine = noStops.translate(None, string.punctuation)\n",
    "    finalLine = finalLine.rstrip()\n",
    "    \n",
    "    return finalLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRawText(inputFile,stopWords):\n",
    "    parsedLines = []\n",
    "    parsedMatrix = []\n",
    "    \n",
    "    for line in inputFile.split(\"\\n\"):\n",
    "        parsedLines.append(line)\n",
    "        \n",
    "    for row in parsedLines:\n",
    "        if row != \"\":\n",
    "            review = preprocessingText(row[:-1],stopWords)\n",
    "            rating = int(row[-1])\n",
    "            parsedMatrix.append([review,rating])\n",
    "    \n",
    "    return parsedMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "stopWords = []\n",
    "keepWords = ['do','doing','but','until','with','against','above','below',\n",
    "             'up','down','in','out','should', \"should've\",'on','off','not',\n",
    "             'over','under',\"wouldn't\",\"won't\",\"won\",\"weren't\",\"were\",\"was\",\n",
    "             \"wasn't\",\"isn't\",\"is\",\"have\",\"haven't\",\"has\",\"hasn't\",\"had\",\n",
    "             \"hadn't\",\"does\",\"doesn't\",\"did\",\"didn't\",\"couldn't\", 'are', \n",
    "             'be','more','most','no','nor',\"don't\", 'been', 'being','having',\n",
    "             \"shouldn't\",'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \n",
    "             \"needn't\", 'shan', \"shan't\", 'shouldn', 'wasn', 'weren', 'wouldn',\n",
    "             'aren', \"aren't\", 'couldn', 'didn','can', 'doesn', 'hadn', 'hasn', \n",
    "             'haven', 'ain', 'isn']\n",
    "\n",
    "for word in stopwords.words('english'):\n",
    "    word = str(word).translate(None, string.punctuation)\n",
    "    try:\n",
    "        part_of_speech = wn.synsets(word)[0].pos()\n",
    "    except:\n",
    "        part_of_speech = 'n'\n",
    "    word = wnl.lemmatize(word,pos=part_of_speech)\n",
    "    word = stemmer.stem(word)\n",
    "    if word not in keepWords:\n",
    "        stopWords.append(word) \n",
    "    \n",
    "\n",
    "    \n",
    "amazonLines = parseRawText(amazonRawText,stopWords)\n",
    "imdbLines = parseRawText(imdbRawText,stopWords)\n",
    "yelpLines = parseRawText(yelpRawText,stopWords)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of creating a single train and test dataset, I split my datasets and testing per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonGood = []\n",
    "imdbGood = []\n",
    "yelpGood = []\n",
    "\n",
    "amazonBad = []\n",
    "imdbBad = []\n",
    "yelpBad = []\n",
    "\n",
    "for good in amazonLines:\n",
    "    if int(good[1]) == 1:\n",
    "        amazonGood.append(good)\n",
    "    else:\n",
    "        amazonBad.append(good)\n",
    "        \n",
    "for good in imdbLines:\n",
    "    if int(good[1]) == 1:\n",
    "        imdbGood.append(good)\n",
    "    else:\n",
    "        imdbBad.append(good)\n",
    "        \n",
    "for good in yelpLines:\n",
    "    if int(good[1]) == 1:\n",
    "        yelpGood.append(good)\n",
    "    else:\n",
    "        yelpBad.append(good)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonGoodTrain = amazonGood[:400]\n",
    "amazonGoodTest = amazonGood[400:]\n",
    "amazonBadTrain = amazonBad[:400]\n",
    "amazonBadTest = amazonBad[400:]\n",
    "\n",
    "imdbGoodTrain = imdbGood[:400]\n",
    "imdbGoodTest = imdbGood[400:]\n",
    "imdbBadTrain = imdbBad[:400]\n",
    "imdbBadTest = imdbBad[400:]\n",
    "\n",
    "yelpGoodTrain = yelpGood[:400]\n",
    "yelpGoodTest = yelpGood[400:]\n",
    "yelpBadTrain = yelpBad[:400]\n",
    "yelpBadTest = yelpBad[400:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonWordList = []\n",
    "imdbWordList = []\n",
    "yelpWordList = []\n",
    "wordList = []\n",
    "\n",
    "#Amazon\n",
    "for line in amazonGoodTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in amazonWordList:\n",
    "            amazonWordList.append(word)\n",
    "            \n",
    "for line in amazonBadTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in amazonWordList:\n",
    "            amazonWordList.append(word)\n",
    "            \n",
    "#Yelp            \n",
    "for line in yelpGoodTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in yelpWordList:\n",
    "            yelpWordList.append(word)\n",
    "            \n",
    "for line in yelpBadTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in yelpWordList:\n",
    "            yelpWordList.append(word)\n",
    "            \n",
    "#IMDB            \n",
    "for line in imdbGoodTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in imdbWordList:\n",
    "            imdbWordList.append(word)\n",
    "            \n",
    "for line in imdbBadTrain:\n",
    "    for word in line[0].split(\" \"):\n",
    "        if word not in imdbWordList:\n",
    "            imdbWordList.append(word)\n",
    "            \n",
    "            \n",
    "for word in amazonWordList:\n",
    "    wordList.append(word)\n",
    "for word in yelpWordList:\n",
    "    if word not in wordList:\n",
    "        wordList.append(word)\n",
    "for word in imdbWordList:\n",
    "    if word not in wordList:\n",
    "        wordList.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonNGramWordList = []\n",
    "imdbNGramWordList = []\n",
    "yelpNGramWordList = []\n",
    "nGramWordList = []\n",
    "span = 2\n",
    "\n",
    "#Amazon\n",
    "for line in amazonGoodTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in amazonNGramWordList:\n",
    "            amazonNGramWordList.append(nGram)\n",
    "            \n",
    "for line in amazonBadTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in amazonNGramWordList:\n",
    "            amazonNGramWordList.append(nGram)\n",
    "            \n",
    "            \n",
    "            \n",
    "#IMDB            \n",
    "for line in imdbGoodTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in imdbNGramWordList:\n",
    "            imdbNGramWordList.append(nGram)\n",
    "            \n",
    "for line in imdbBadTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in imdbNGramWordList:\n",
    "            imdbNGramWordList.append(nGram)\n",
    "            \n",
    "            \n",
    "\n",
    "#Yelp\n",
    "for line in yelpGoodTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in yelpNGramWordList:\n",
    "            yelpNGramWordList.append(nGram)\n",
    "            \n",
    "for line in yelpBadTrain:\n",
    "    words = line[0].split(\" \")\n",
    "    z = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "    for nGram in z:\n",
    "        if nGram not in yelpNGramWordList:\n",
    "            yelpNGramWordList.append(nGram)\n",
    "            \n",
    "            \n",
    "            \n",
    "for word in amazonNGramWordList:\n",
    "    nGramWordList.append(word)\n",
    "for word in yelpNGramWordList:\n",
    "    if word not in wordList:\n",
    "        nGramWordList.append(word)\n",
    "for word in imdbNGramWordList:\n",
    "    if word not in wordList:\n",
    "        nGramWordList.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVectors(inputMatrix,chosenWordList):\n",
    "    outputMatrix = []\n",
    "    for line in inputMatrix:\n",
    "        tempLine = [0]*len(chosenWordList)\n",
    "        for word in line[0].split(\" \"):\n",
    "            if word in chosenWordList:\n",
    "                index = chosenWordList.index(word)\n",
    "                tempLine[index]+=1\n",
    "        outputMatrix.append(tempLine)\n",
    "        \n",
    "    return outputMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGramsFeatureVectors(inputMatrix,chosenWordList):\n",
    "    outputMatrix = []\n",
    "    for line in inputMatrix:\n",
    "        tempLine = [0]*len(chosenWordList)\n",
    "        words = line[0].split(\" \")\n",
    "        nGrams = [\" \".join(words[i:i+span]) for i in range(0, len(words)-1)]\n",
    "        for word in nGrams:\n",
    "            if word in chosenWordList:\n",
    "                index = chosenWordList.index(word)\n",
    "                tempLine[index]+=1\n",
    "        outputMatrix.append(tempLine)\n",
    "        \n",
    "    return outputMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "amazonFeatureGoodTrain = featureVectors(amazonGoodTrain,amazonWordList)\n",
    "amazonFeatureGoodTest = featureVectors(amazonGoodTest,amazonWordList)\n",
    "amazonFeatureBadTrain = featureVectors(amazonBadTrain,amazonWordList)\n",
    "amazonFeatureBadTest = featureVectors(amazonBadTest,amazonWordList)\n",
    "\n",
    "imdbFeatureGoodTrain = featureVectors(imdbGoodTrain,imdbWordList)\n",
    "imdbFeatureGoodTest = featureVectors(imdbGoodTest,imdbWordList)\n",
    "imdbFeatureBadTrain = featureVectors(imdbBadTrain,imdbWordList)\n",
    "imdbFeatureBadTest = featureVectors(imdbBadTest,imdbWordList)\n",
    "\n",
    "yelpFeatureGoodTrain = featureVectors(yelpGoodTrain,yelpWordList)\n",
    "yelpFeatureGoodTest = featureVectors(yelpGoodTest,yelpWordList)\n",
    "yelpFeatureBadTrain = featureVectors(yelpBadTrain,yelpWordList)\n",
    "yelpFeatureBadTest = featureVectors(yelpBadTest,yelpWordList)\n",
    "\n",
    "print \"Done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print amazonFeatureGoodTrain[0]\n",
    "print yelpFeatureBadTrain[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobNorm(inputMatrix):\n",
    "    normVal = []\n",
    "    normArray = []\n",
    "    inputMatrix = numpy.asarray(inputMatrix)\n",
    "    transposeMatrix = inputMatrix.T\n",
    "    for line in transposeMatrix:\n",
    "        normVal.append(numpy.linalg.norm(line))\n",
    "    \n",
    "    for line in inputMatrix:\n",
    "        normArray.append(numpy.multiply(line,normVal))\n",
    "        \n",
    "    return normArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewLogReg(wordList,goodTrain,badTrain,goodTest,badTest,featureGoodTrain,featureBadTrain,featureGoodTest,featureBadTest):\n",
    "    trainLabels = []\n",
    "    for line in goodTrain:\n",
    "        trainLabels.append(line[1])\n",
    "    for line in badTrain:\n",
    "        trainLabels.append(line[1])\n",
    "\n",
    "    trainData = numpy.concatenate((frobNorm(featureGoodTrain),frobNorm(featureBadTrain)))\n",
    "\n",
    "\n",
    "    testLabels = []\n",
    "    for line in goodTest:\n",
    "        testLabels.append(line[1])\n",
    "    for line in badTest:\n",
    "        testLabels.append(line[1])\n",
    "\n",
    "    testData = numpy.concatenate((frobNorm(featureGoodTest),featureBadTest))\n",
    "\n",
    "\n",
    "    logReg = LogisticRegression()\n",
    "    y_pred1 = logReg.fit(trainData, trainLabels)\n",
    "    #training binary values, training encoded labels\n",
    "    logRegScore = y_pred1.score(testData, testLabels)\n",
    "    #testing binarvalues, testing encoded labels\n",
    "    coefficients = logReg.coef_\n",
    "    coef_dict = {}\n",
    "    for i,coef in enumerate(coefficients[0]):\n",
    "        coef_dict[i] = coef\n",
    "    sortedList = sorted(coef_dict.items(), key=lambda kv:kv[1], reverse=True)\n",
    "    for tuple in sortedList[:10]:\n",
    "        print wordList[tuple[0]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    GNB = GaussianNB()\n",
    "    y_pred2 = GNB.fit(trainData,trainLabels)\n",
    "    GNBScore = y_pred2.score(testData,testLabels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"LogReg:\"\n",
    "    print confusion_matrix(y_pred1.predict(testData),testLabels)\n",
    "    print \"GNB:\"\n",
    "    print confusion_matrix(y_pred2.predict(testData),testLabels)\n",
    "    \n",
    "    \n",
    "    return logRegScore,GNBScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "without\n",
      "best\n",
      "nice\n",
      "easy\n",
      "everything\n",
      "order\n",
      "comfortable\n",
      "price\n",
      "excellent\n",
      "LogReg:\n",
      "[[95 29]\n",
      " [ 5 71]]\n",
      "GNB:\n",
      "[[58 21]\n",
      " [42 79]]\n",
      "(0.83, 0.685)\n"
     ]
    }
   ],
   "source": [
    "print reviewLogReg(amazonWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,amazonFeatureGoodTrain,amazonFeatureBadTrain,amazonFeatureGoodTest,amazonFeatureBadTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic\n",
      "fun\n",
      "awesome\n",
      "check\n",
      "nice\n",
      "delicious\n",
      "loved\n",
      "happy\n",
      "tender\n",
      "first\n",
      "LogReg:\n",
      "[[84 21]\n",
      " [16 79]]\n",
      "GNB:\n",
      "[[53 13]\n",
      " [47 87]]\n",
      "(0.815, 0.7)\n"
     ]
    }
   ],
   "source": [
    "print reviewLogReg(yelpWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,yelpFeatureGoodTrain,yelpFeatureBadTrain,yelpFeatureGoodTest,yelpFeatureBadTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liked\n",
      "right\n",
      "awesome\n",
      "watch\n",
      "funny\n",
      "beautiful\n",
      "loved\n",
      "nice\n",
      "1010\n",
      "love\n",
      "LogReg:\n",
      "[[89 29]\n",
      " [11 71]]\n",
      "GNB:\n",
      "[[85 36]\n",
      " [15 64]]\n",
      "(0.8, 0.745)\n"
     ]
    }
   ],
   "source": [
    "print reviewLogReg(imdbWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,imdbFeatureGoodTrain,imdbFeatureBadTrain,imdbFeatureGoodTest,imdbFeatureBadTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGramLogReg(nGramWordList,goodTrain,badTrain,goodTest,badTest,featureGoodTrain,featureBadTrain,featureGoodTest,featureBadTest):\n",
    "    trainLabels = []\n",
    "    for line in goodTrain:\n",
    "        trainLabels.append(line[1])\n",
    "    for line in badTrain:\n",
    "        trainLabels.append(line[1])\n",
    "\n",
    "    trainData = numpy.concatenate((frobNorm(featureGoodTrain),frobNorm(featureBadTrain)))\n",
    "\n",
    "\n",
    "    testLabels = []\n",
    "    for line in goodTest:\n",
    "        testLabels.append(line[1])\n",
    "    for line in badTest:\n",
    "        testLabels.append(line[1])\n",
    "\n",
    "    testData = numpy.concatenate((featureGoodTest,featureBadTest))\n",
    "\n",
    "\n",
    "    logReg = LogisticRegression()\n",
    "    y_pred1 = logReg.fit(trainData, trainLabels)\n",
    "    #training binary values, training encoded labels\n",
    "    logRegScore = y_pred1.score(testData, testLabels)\n",
    "    #testing binarvalues, testing encoded labels\n",
    "    coefficients = logReg.coef_\n",
    "    coef_dict = {}\n",
    "    for i,coef in enumerate(coefficients[0]):\n",
    "        coef_dict[i] = coef\n",
    "    sortedList = sorted(coef_dict.items(), key=lambda kv:kv[1], reverse=True)\n",
    "    for tuple in sortedList[:10]:\n",
    "        print nGramWordList[tuple[0]]\n",
    "    \n",
    "    GNB = GaussianNB()\n",
    "    y_pred2 = GNB.fit(trainData,trainLabels)\n",
    "    GNBScore = y_pred2.score(testData,testLabels)\n",
    "    \n",
    "    print \"LogReg:\"\n",
    "    print confusion_matrix(y_pred1.predict(testData),testLabels)\n",
    "    print \"GNB:\"\n",
    "    print confusion_matrix(y_pred2.predict(testData),testLabels)\n",
    "    \n",
    "    return logRegScore,GNBScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "amazonNGramFeatureGoodTrain = nGramsFeatureVectors(amazonGoodTrain,amazonNGramWordList)\n",
    "amazonNGramFeatureGoodTest = nGramsFeatureVectors(amazonGoodTest,amazonNGramWordList)\n",
    "amazonNGramFeatureBadTrain = nGramsFeatureVectors(amazonBadTrain,amazonNGramWordList)\n",
    "amazonNGramFeatureBadTest = nGramsFeatureVectors(amazonBadTest,amazonNGramWordList)\n",
    "\n",
    "imdbNGramFeatureGoodTrain = nGramsFeatureVectors(imdbGoodTrain,imdbNGramWordList)\n",
    "imdbNGramFeatureGoodTest = nGramsFeatureVectors(imdbGoodTest,imdbNGramWordList)\n",
    "imdbNGramFeatureBadTrain = nGramsFeatureVectors(imdbBadTrain,imdbNGramWordList)\n",
    "imdbNGramFeatureBadTest = nGramsFeatureVectors(imdbBadTest,imdbNGramWordList)\n",
    "\n",
    "yelpNGramFeatureGoodTrain = nGramsFeatureVectors(yelpGoodTrain,yelpNGramWordList)\n",
    "yelpNGramFeatureGoodTest = nGramsFeatureVectors(yelpGoodTest,yelpNGramWordList)\n",
    "yelpNGramFeatureBadTrain = nGramsFeatureVectors(yelpBadTrain,yelpNGramWordList)\n",
    "yelpNGramFeatureBadTest = nGramsFeatureVectors(yelpBadTest,yelpNGramWordList)\n",
    "\n",
    "print \"Done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works great\n",
      "great phone\n",
      "easy use\n",
      "highly recommend\n",
      "works well\n",
      "is excellent\n",
      "great product\n",
      "would recommend\n",
      "up well\n",
      "is great\n",
      "LogReg:\n",
      "[[95 58]\n",
      " [ 5 42]]\n",
      "GNB:\n",
      "[[34  9]\n",
      " [66 91]]\n",
      "(0.685, 0.625)\n"
     ]
    }
   ],
   "source": [
    "print nGramLogReg(amazonNGramWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,amazonNGramFeatureGoodTrain,amazonNGramFeatureBadTrain,amazonNGramFeatureGoodTest,amazonNGramFeatureBadTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 out\n",
      "is good\n",
      "is great\n",
      "ray charles\n",
      "excellent performance\n",
      "give one\n",
      "one best\n",
      "gave 10\n",
      "are good\n",
      "saw film\n",
      "LogReg:\n",
      "[[88 70]\n",
      " [12 30]]\n",
      "GNB:\n",
      "[[91 73]\n",
      " [ 9 27]]\n",
      "(0.59, 0.59)\n"
     ]
    }
   ],
   "source": [
    "print nGramLogReg(imdbNGramWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,imdbNGramFeatureGoodTrain,imdbNGramFeatureBadTrain,imdbNGramFeatureGoodTest,imdbNGramFeatureBadTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really good\n",
      "food delicious\n",
      "is good\n",
      "great place\n",
      "in town\n",
      "food good\n",
      "great food\n",
      "definitely be\n",
      "friendly staff\n",
      "check out\n",
      "LogReg:\n",
      "[[27 13]\n",
      " [73 87]]\n",
      "GNB:\n",
      "[[24  9]\n",
      " [76 91]]\n",
      "(0.57, 0.575)\n"
     ]
    }
   ],
   "source": [
    "print nGramLogReg(yelpNGramWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,yelpNGramFeatureGoodTrain,yelpNGramFeatureBadTrain,yelpNGramFeatureGoodTest,yelpNGramFeatureBadTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part H "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices are avoided simply to save space, please refer above to method used for printing confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeMatrix(inputMatrix,mean=\"skip\"):\n",
    "    inputMatrix = numpy.asarray(inputMatrix)\n",
    "    if mean == \"skip\":\n",
    "        mean = inputMatrix.mean()\n",
    "    centeredMatrix = inputMatrix - mean\n",
    "    return centeredMatrix,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaMatrixMaker(inputTrainMatrix,inputTestMatrix,k):\n",
    "    centeredTrainMatrix,avg = standardizeMatrix(inputTrainMatrix)\n",
    "    U,D,Vt = numpy.linalg.svd(centeredTrainMatrix)\n",
    "    Vt = numpy.asarray(Vt)\n",
    "    pcaTrainMatrix = numpy.matmul(centeredTrainMatrix,Vt.T[:,:k])\n",
    "    \n",
    "    centeredTestMatrix,avg = standardizeMatrix(inputTestMatrix)\n",
    "    pcaTestMatrix = numpy.matmul(centeredTestMatrix,Vt.T[:,:k])\n",
    "    \n",
    "    \n",
    "    return pcaTrainMatrix, pcaTestMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Bag-of-Words: (LogReg,GNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n",
      "great\n",
      "razr\n",
      "jawbone\n",
      "case\n",
      "mic\n",
      "is\n",
      "good\n",
      "excellent\n",
      "value\n",
      "LogReg:\n",
      "[[91 59]\n",
      " [ 9 41]]\n",
      "GNB:\n",
      "[[97 75]\n",
      " [ 3 25]]\n",
      "(0.66, 0.61)\n",
      "\n",
      "are\n",
      "combination\n",
      "razr\n",
      "use\n",
      "three\n",
      "recommend\n",
      "great\n",
      "going\n",
      "blue\n",
      "jawbone\n",
      "LogReg:\n",
      "[[90 47]\n",
      " [10 53]]\n",
      "GNB:\n",
      "[[89 49]\n",
      " [11 51]]\n",
      "(0.715, 0.7)\n",
      "\n",
      "well\n",
      "actually\n",
      "seems\n",
      "car\n",
      "ac\n",
      "charger\n",
      "bulky\n",
      "are\n",
      "2mp\n",
      "mobile\n",
      "LogReg:\n",
      "[[92 42]\n",
      " [ 8 58]]\n",
      "GNB:\n",
      "[[  0   1]\n",
      " [100  99]]\n",
      "(0.75, 0.495)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((amazonFeatureGoodTrain,amazonFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonFeatureGoodTest,amazonFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = reviewLogReg(amazonWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((amazonFeatureGoodTrain,amazonFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonFeatureGoodTest,amazonFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = reviewLogReg(amazonWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((amazonFeatureGoodTrain,amazonFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonFeatureGoodTest,amazonFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = reviewLogReg(amazonWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song\n",
      "keeps\n",
      "trying\n",
      "gerardo\n",
      "scene\n",
      "best\n",
      "find\n",
      "movie\n",
      "in\n",
      "is\n",
      "LogReg:\n",
      "[[61 51]\n",
      " [39 49]]\n",
      "GNB:\n",
      "[[57 49]\n",
      " [43 51]]\n",
      "(0.55, 0.54)\n",
      "\n",
      "effort\n",
      "face\n",
      "adorable\n",
      "case\n",
      "saw\n",
      "song\n",
      "hilarious\n",
      "messages\n",
      "showed\n",
      "keeps\n",
      "LogReg:\n",
      "[[73 49]\n",
      " [27 51]]\n",
      "GNB:\n",
      "[[100 100]\n",
      " [  0   0]]\n",
      "(0.62, 0.5)\n",
      "\n",
      "words\n",
      "face\n",
      "think\n",
      "effort\n",
      "film\n",
      "constructed\n",
      "case\n",
      "two\n",
      "cinema\n",
      "adorable\n",
      "LogReg:\n",
      "[[74 41]\n",
      " [26 59]]\n",
      "GNB:\n",
      "[[100 100]\n",
      " [  0   0]]\n",
      "(0.665, 0.5)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((imdbFeatureGoodTrain,imdbFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbFeatureGoodTest,imdbFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = reviewLogReg(imdbWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((imdbFeatureGoodTrain,imdbFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbFeatureGoodTest,imdbFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = reviewLogReg(imdbWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((imdbFeatureGoodTrain,imdbFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbFeatureGoodTest,imdbFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = reviewLogReg(imdbWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may\n",
      "loved\n",
      "late\n",
      "wow\n",
      "rick\n",
      "holiday\n",
      "place\n",
      "bank\n",
      "stopped\n",
      "off\n",
      "LogReg:\n",
      "[[72 42]\n",
      " [28 58]]\n",
      "GNB:\n",
      "[[78 59]\n",
      " [22 41]]\n",
      "(0.65, 0.595)\n",
      "\n",
      "cod\n",
      "highly\n",
      "food\n",
      "cute\n",
      "interior\n",
      "cape\n",
      "is\n",
      "care\n",
      "velvet\n",
      "may\n",
      "LogReg:\n",
      "[[78 40]\n",
      " [22 60]]\n",
      "GNB:\n",
      "[[  1   0]\n",
      " [ 99 100]]\n",
      "(0.69, 0.505)\n",
      "\n",
      "provided\n",
      "delight\n",
      "combos\n",
      "cod\n",
      "mexican\n",
      "not\n",
      "server\n",
      "highly\n",
      "right\n",
      "be\n",
      "LogReg:\n",
      "[[85 30]\n",
      " [15 70]]\n",
      "GNB:\n",
      "[[  0   0]\n",
      " [100 100]]\n",
      "(0.775, 0.5)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((yelpFeatureGoodTrain,yelpFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpFeatureGoodTest,yelpFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = reviewLogReg(yelpWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((yelpFeatureGoodTrain,yelpFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpFeatureGoodTest,yelpFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = reviewLogReg(yelpWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((yelpFeatureGoodTrain,yelpFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpFeatureGoodTest,yelpFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = reviewLogReg(yelpWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Grams: (LogReg,GNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great jawbone\n",
      "razr owneryou\n",
      "case excellent\n",
      "mic is\n",
      "must have\n",
      "owneryou must\n",
      "is great\n",
      "good case\n",
      "excellent value\n",
      "are razr\n",
      "LogReg:\n",
      "[[100  85]\n",
      " [  0  15]]\n",
      "GNB:\n",
      "[[100  96]\n",
      " [  0   4]]\n",
      "(0.575, 0.52)\n",
      "\n",
      "owneryou must\n",
      "phone 7\n",
      "must have\n",
      "fire absolutely\n",
      "without charging\n",
      "blue tooth\n",
      "mic is\n",
      "extended battery\n",
      "far good\n",
      "is great\n",
      "LogReg:\n",
      "[[100  93]\n",
      " [  0   7]]\n",
      "GNB:\n",
      "[[100  99]\n",
      " [  0   1]]\n",
      "(0.535, 0.505)\n",
      "\n",
      "charger well\n",
      "excellent bluetooth\n",
      "clear with\n",
      "are sensitive\n",
      "well ac\n",
      "car charger\n",
      "nice headset\n",
      "owneryou must\n",
      "case is\n",
      "phone ive\n",
      "LogReg:\n",
      "[[99 87]\n",
      " [ 1 13]]\n",
      "GNB:\n",
      "[[  0   0]\n",
      " [100 100]]\n",
      "(0.56, 0.5)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((amazonNGramFeatureGoodTrain,amazonNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonNGramFeatureGoodTest,amazonNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = nGramLogReg(amazonNGramWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((amazonFeatureGoodTrain,amazonFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonFeatureGoodTest,amazonFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = nGramLogReg(amazonNGramWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((amazonFeatureGoodTrain,amazonFeatureBadTrain))\n",
    "testConc = numpy.concatenate((amazonFeatureGoodTest,amazonFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = nGramLogReg(amazonNGramWordList,amazonGoodTrain,amazonBadTrain,amazonGoodTest,amazonBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeps running\n",
      "movie gerardo\n",
      "in movie\n",
      "best scene\n",
      "trying find\n",
      "scene in\n",
      "gerardo is\n",
      "is trying\n",
      "find song\n",
      "song keeps\n",
      "LogReg:\n",
      "[[ 6  7]\n",
      " [94 93]]\n",
      "GNB:\n",
      "[[97 92]\n",
      " [ 3  8]]\n",
      "(0.495, 0.525)\n",
      "\n",
      "running head\n",
      "muppets were\n",
      "almost right\n",
      "right on\n",
      "review is\n",
      "saw movie\n",
      "science teacher\n",
      "is right\n",
      "were best\n",
      "trying find\n",
      "LogReg:\n",
      "[[22 13]\n",
      " [78 87]]\n",
      "GNB:\n",
      "[[99 97]\n",
      " [ 1  3]]\n",
      "(0.545, 0.51)\n",
      "\n",
      "almost right\n",
      "editing directing\n",
      "easily most\n",
      "muppets were\n",
      "overdue since\n",
      "think no\n",
      "review is\n",
      "constructed in\n",
      "cinematography acting\n",
      "delivers everything\n",
      "LogReg:\n",
      "[[85 77]\n",
      " [15 23]]\n",
      "GNB:\n",
      "[[98 98]\n",
      " [ 2  2]]\n",
      "(0.54, 0.5)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((imdbNGramFeatureGoodTrain,imdbNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbNGramFeatureGoodTest,imdbNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = nGramLogReg(imdbNGramWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((imdbNGramFeatureGoodTrain,imdbNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbNGramFeatureGoodTest,imdbNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = nGramLogReg(imdbNGramWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((imdbNGramFeatureGoodTrain,imdbNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((imdbNGramFeatureGoodTest,imdbNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = nGramLogReg(imdbNGramWordList,imdbGoodTrain,imdbBadTrain,imdbGoodTest,imdbBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late may\n",
      "rick steve\n",
      "holiday off\n",
      "may bank\n",
      "wow loved\n",
      "stopped late\n",
      "loved place\n",
      "off rick\n",
      "bank holiday\n",
      "steve recommendation\n",
      "LogReg:\n",
      "[[16  6]\n",
      " [84 94]]\n",
      "GNB:\n",
      "[[10  7]\n",
      " [90 93]]\n",
      "(0.55, 0.515)\n",
      "\n",
      "tacos friendly\n",
      "also cute\n",
      "cakeohhh stuff\n",
      "cape cod\n",
      "were great\n",
      "street tacos\n",
      "late may\n",
      "velvet cakeohhh\n",
      "holiday off\n",
      "great touch\n",
      "LogReg:\n",
      "[[17  9]\n",
      " [83 91]]\n",
      "GNB:\n",
      "[[10  5]\n",
      " [90 95]]\n",
      "(0.54, 0.525)\n",
      "\n",
      "tacos friendly\n",
      "place lot\n",
      "visit hiro\n",
      "also cute\n",
      "combos like\n",
      "were great\n",
      "first visit\n",
      "note server\n",
      "cape cod\n",
      "velvet cakeohhh\n",
      "LogReg:\n",
      "[[19 11]\n",
      " [81 89]]\n",
      "GNB:\n",
      "[[12  5]\n",
      " [88 95]]\n",
      "(0.54, 0.535)\n"
     ]
    }
   ],
   "source": [
    "trainConc = numpy.concatenate((yelpNGramFeatureGoodTrain,yelpNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpNGramFeatureGoodTest,yelpNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,10)\n",
    "score = nGramLogReg(yelpNGramWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((yelpNGramFeatureGoodTrain,yelpNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpNGramFeatureGoodTest,yelpNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,50)\n",
    "score = nGramLogReg(yelpNGramWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score\n",
    "\n",
    "print\n",
    "\n",
    "trainConc = numpy.concatenate((yelpNGramFeatureGoodTrain,yelpNGramFeatureBadTrain))\n",
    "testConc = numpy.concatenate((yelpNGramFeatureGoodTest,yelpNGramFeatureBadTest))\n",
    "A,B = pcaMatrixMaker(trainConc,testConc,100)\n",
    "score = nGramLogReg(yelpNGramWordList,yelpGoodTrain,yelpBadTrain,yelpGoodTest,yelpBadTest,A[:400],A[400:],B[:400],B[400:])    \n",
    "print score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
